{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyORfH729w2Wiga2vYHqDwkF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish2Parimi/Telugu_Headline_Generator/blob/main/Sub%20word%20token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44kFwM8-ozq",
        "outputId": "5a503fed-1e66-4ac2-aa9f-1c1f56102e34"
      },
      "source": [
        "# `BertTokenizer.detokenize` is not in `tf-text` stable yet (currently 2.4.3).\n",
        "!pip install -q tensorflow_text_nightly\n",
        "# tf-text-nightly resquires tf-nightly\n",
        "!pip install -q tf-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 455.5MB 40kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 49.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.1MB 21.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 54.2MB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlg1fkPJDrgQ"
      },
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVzCzYZbD_6S"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/data.csv',encoding ='UTF-8')\n",
        "\n",
        "# data = data.drop(['SNo','date','topic'],axis = 1)\n",
        "data.dropna(axis = 0,inplace = True)\n",
        "\n",
        "# data = data.applymap(lambda x:x.encode(encoding='UTF-8'))\n",
        "\n",
        "\n",
        "def format_data(data):\n",
        "   \n",
        "    \n",
        "    # Tokenize text\n",
        "    data = data.applymap(lambda x: str(x) if isinstance(x, int) or isinstance(x, float) else x)\n",
        "    data['heading'] = data['heading'].str.replace('\\d+', '')\n",
        "    # data['heading'] = data['heading'].apply(lambda x:vect_dvect.vect_2(x,False,maxlen,max_features))\n",
        "\n",
        "    data['body'] = data['body'].str.replace('\\d+', '')\n",
        "    # data['body'] = data['body'].apply(lambda x:vect_dvect.vect_2(x,True,maxlen,max_features))\n",
        "\n",
        "    # data = data.applymap(lambda x:vect(x))\n",
        "    X = data['body'].tolist()\n",
        "    Y = data['heading'].tolist()\n",
        "    \n",
        "    return X, Y\n",
        "\n",
        "X, Y = format_data(data)\n",
        "target = data.pop('heading')\n",
        "data = data.pop('body')\n",
        "\n",
        "\n",
        "train_examples = tf.data.Dataset.from_tensor_slices((data.values, target.values))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfLBsQnyJ7tn",
        "outputId": "a9b7d0de-5fb0-46f4-afd3-f3e5d0ed0220"
      },
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portuguese:  భారీ ఎత్తున మొండిబకాయిలు పెరిగిపోవడంతో ఐడిబిఐ వ్యవహారాలపై భారత రిజర్వు బ్యాంకు (ఆర్‌బిఐ) ఒక కన్నేసింది. ఐడిబిఐ బ్యాంకును ఆర్‌బిఐ వాచ్‌లి్‌స్టలో చేర్చినట్టుగా తెలిసింది. నికర మొండిపద్దుల మొత్తం 6 శాతం దాటడం, వరసగా రెండేళ్ల పాటు నష్టాలను  ప్రకటించడం, కాపిటల్‌ అడెక్వసీ నిర్దేశిత ప్రమాణాల కంటే తగ్గడం... ఈ సందర్భాల్లో బ్యాంకులను ఆర్‌బిఐ వాచ్‌ లిస్ట్‌లో చేరుస్తుంది. తమ బ్యాంకుకు సంబంధించి ఆర్‌బిఐ ప్రాంప్ట్‌ కరెక్టీవ్‌ యాక్షన్‌ (పిసిఎ) చేపట్టినట్టు ఐడిబిఐ వెల్లడించింది. గతంలో ఓవర్సీస్‌ బ్యాంక్‌, యునైటెడ్‌ బ్యాంక్‌కు సంబంధించి  కూడా ఆర్‌బిఐ పిసిఎ చేపట్టింది. పిసిఎ వల్ల బ్యాంకుపై ఆర్థికంగా భారం పడదనీ, దీనివల్ల అంతర్గత నియంత్రణలు మెరుగుపడటంతో పాటు, కార్యకలాపాలు మెరుగవుతాయని ఐడిబిఐ వివరించింది. ఆర్‌బిఐ ఆదేశాలకు అనుగుణంగా ఇప్పుడు మూలధనాన్ని పొదుపుగా వినియోగించడంపై  ఐడిబిఐ దృష్టి సారించాల్సి ఉంటుంది.ఇందులో భాగంగా పరపతిపై నియంత్రణలు విధించడం, నియామకాలను నిలిపేయడం, కొత్త పెట్టుబడి పథకాలను పక్కన బెట్టడంవంటి చర్యలు తీసుకోవాల్సి ఉంటుం ది. బ్యాంకు సరైన రీతిలో స్పందించని పక్షంలో మరో బలమైన బ్యాంకులో  విలీనానికి  సిద్ధం కావాల్సి ఉంటుంది.\n",
            "English:    ఐడిబిఐపై ఆర్‌బిఐ నజర్‌\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgNKVxieL1x9"
      },
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHLZz9KdGHje",
        "outputId": "93a8c167-fefd-49c0-fd48-4adb9352803c"
      },
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy().decode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ఐడిబిఐపై ఆర్‌బిఐ నజర్‌\n",
            "బ్యాంకింగ్‌ చీఫ్‌లతో నేడు జైట్లీ భేటీ\n",
            "కీలక వికెట్ తీసిన జడేజా..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9r9OdwkLB1M"
      },
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcVpfz-WIGzL"
      },
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HDW2JH_L42N",
        "outputId": "ea22fc27-5f07-414f-98c3-28a8d4421099"
      },
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 28s, sys: 3.55 s, total: 11min 31s\n",
            "Wall time: 11min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWZQbGu3L8cX",
        "outputId": "05cce993-682b-457f-a40e-b2bff3771430"
      },
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '%', '&', \"'\"]\n",
            "['ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర']\n",
            "['బయటకు', 'చతల', '##మద', 'నటుడు', '##ుకవడం', 'పలువురు', '##కుల', '##నకు', 'నజం', 'అత']\n",
            "['##౩', '##౪', '##౫', '##౭', '##౯', '##–', '##‘', '##’', '##“', '##”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnKKDzXEPy3g"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dThIacV5Q9O0"
      },
      "source": [
        "write_vocab_file('pt_vocab.txt', pt_vocab)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZbZxL45RCHd",
        "outputId": "26632a27-e122-4530-99fe-dc6f4dd8cc94"
      },
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37.5 s, sys: 235 ms, total: 37.7 s\n",
            "Wall time: 37.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wje3fqPRRPG",
        "outputId": "00a76f9d-db21-43d7-db18-22fd8bc76eb5"
      },
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '%', \"'\", '(', ')']\n",
            "['–', '‘', '’', '##ల', '##న', '##కు', '##ప', '##త', '##క', '##ు']\n",
            "['##దప', '##దరులకు', '##దుకు', '##ధలు', '##పరట', '##భం', '##మద', '##యక', '##లనూ', '##హలు']\n",
            "['##ఙ', '##ఛ', '##ఝ', '##ఞ', '##ఢ', '##ృ', '##ౄ', '##–', '##‘', '##’']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5EHVQyXRWyb"
      },
      "source": [
        "write_vocab_file('en_vocab.txt', en_vocab)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv5nZ_aCRgZD"
      },
      "source": [
        "pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1CFbXl9Rhvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43218865-e752-4f10-ff1c-e05c5f299797"
      },
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy().decode('utf-8'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ఐడిబిఐపై ఆర్‌బిఐ నజర్‌\n",
            "బ్యాంకింగ్‌ చీఫ్‌లతో నేడు జైట్లీ భేటీ\n",
            "కీలక వికెట్ తీసిన జడేజా..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHSM4m6URqcr",
        "outputId": "da74d6e9-19f6-4243-a167-bf5a1fb2ee0e"
      },
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[58, 117, 171, 2350, 538, 1941]\n",
            "[1674, 409, 215, 304, 382, 364]\n",
            "[302, 1215, 1789, 1427, 13, 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560laVRlR8hH",
        "outputId": "bb18ff87-1e9c-4799-ce7d-c7ad60f28e23"
      },
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'\\xe0\\xb0\\x90 ##\\xe0\\xb0\\xa1 ##\\xe0\\xb0\\xac ##\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0',\n",
              "       b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab ##\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f',\n",
              "       b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . .'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Yf3nLzSANp",
        "outputId": "90f66796-541f-4e41-9eaa-304b57acc720"
      },
      "source": [
        "words = en_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'\\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0',\n",
              "       b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f',\n",
              "       b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . .'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRMlxbqzSFqm"
      },
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IicFn72VSIhC",
        "outputId": "d3c20661-69d1-4008-fae8-30f82a85c308"
      },
      "source": [
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'[START] \\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0 [END]',\n",
              "       b'[START] \\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f [END]',\n",
              "       b'[START] \\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . . [END]'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNKnUkL9SMiD"
      },
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDLkJ84dSO1J"
      },
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79CxtKY3SZA9",
        "outputId": "bacc2180-797e-4f19-e24b-e53f8ecfc49b"
      },
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'\\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0',\n",
              "       b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f',\n",
              "       b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . .'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu-nToxaSgAM"
      },
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:   \n",
        "\n",
        "    # Include a tokenize signature for a batch of strings. \n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSKfX2BcShtL",
        "outputId": "e96f4358-606f-494d-862f-45f775fa2a19"
      },
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.bd = CustomTokenizer(reserved_tokens, 'pt_vocab.txt')\n",
        "tokenizers.hd = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NoFy3foSu6-",
        "outputId": "61162764-c4e6-4339-9cb1-37f57590eb43"
      },
      "source": [
        "model_name = 'telugu_tokenizer'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: telugu_tokenizer/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF2UEPPPTDGq",
        "outputId": "ea27f744-7d0b-4055-f8ab-cd5cb188df64"
      },
      "source": [
        "!zip -r {model_name}.zip {model_name}"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: telugu_tokenizer/ (stored 0%)\n",
            "  adding: telugu_tokenizer/saved_model.pb (deflated 91%)\n",
            "  adding: telugu_tokenizer/variables/ (stored 0%)\n",
            "  adding: telugu_tokenizer/variables/variables.index (deflated 33%)\n",
            "  adding: telugu_tokenizer/variables/variables.data-00000-of-00001 (deflated 69%)\n",
            "  adding: telugu_tokenizer/assets/ (stored 0%)\n",
            "  adding: telugu_tokenizer/assets/en_vocab.txt (deflated 71%)\n",
            "  adding: telugu_tokenizer/assets/pt_vocab.txt (deflated 72%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Ycc53sTVX2",
        "outputId": "7a5b90b1-cbb6-41c6-a63e-a4fe96aa023a"
      },
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
        "reloaded_tokenizers.bd.get_vocab_size().numpy()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db1GU80uTafF",
        "outputId": "ae6dd2dc-34b5-4d94-eec9-4a6896e64848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = reloaded_tokenizers.hd.tokenize(['బ్యాంకింగ్‌ చీఫ్‌లతో నేడు జైట్లీ భేటీ'])\n",
        "tokens.numpy()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2, 1674,  409,  215,  304,  382,  364,    3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2BgiGsRU3Ve",
        "outputId": "bfcfeb7f-8b05-4e59-854e-cb2929f14ea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_tokens = reloaded_tokenizers.hd.lookup(tokens)\n",
        "text_tokens"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97', b'\\xe0\\xb0\\x9a\\xe0\\xb0\\xab', b'##\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4', b'\\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81', b'\\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xad\\xe0\\xb0\\x9f', b'[END]']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loqllkksU_LJ",
        "outputId": "d9b17d6b-c31c-495d-ec3a-694283ee0c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "round_trip = reloaded_tokenizers.hd.detokenize(tokens)\n",
        "\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "బయంకంగ చఫలత నడు జటల భట\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}