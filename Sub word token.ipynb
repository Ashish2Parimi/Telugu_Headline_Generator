{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyOJYlcU7eu7X+965FkIlNAb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish2Parimi/Telugu_Headline_Generator/blob/main/Sub%20word%20token.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44kFwM8-ozq",
        "outputId": "338bcb66-7e3f-4c30-a4bb-3e2f4644575b"
      },
      "source": [
        "# `BertTokenizer.detokenize` is not in `tf-text` stable yet (currently 2.4.3).\n",
        "!pip install -q tensorflow_text_nightly\n",
        "# tf-text-nightly resquires tf-nightly\n",
        "!pip install -q tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 455.5MB 38kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 36.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.1MB 32.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 53.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 26.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 45.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 42.4MB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlg1fkPJDrgQ"
      },
      "source": [
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSIae39zDc2I"
      },
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nieixabhDxX9"
      },
      "source": [
        "tokenizer = text.BertTokenizer('vocab.txt', **bert_tokenizer_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVzCzYZbD_6S"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/train_telugu_news.csv',encoding ='UTF-8')\n",
        "\n",
        "data = data.drop(['SNo','date','topic'],axis = 1)\n",
        "data.dropna(axis = 0,inplace = True)\n",
        "\n",
        "# data = data.applymap(lambda x:x.encode(encoding='UTF-8'))\n",
        "\n",
        "maxlen = 1024\n",
        "max_features = 10\n",
        "def format_data(data):\n",
        "   \n",
        "    \n",
        "    # Tokenize text\n",
        "    data = data.applymap(lambda x: str(x) if isinstance(x, int) or isinstance(x, float) else x)\n",
        "    data['heading'] = data['heading'].str.replace('\\d+', '')\n",
        "    # data['heading'] = data['heading'].apply(lambda x:vect_dvect.vect_2(x,False,maxlen,max_features))\n",
        "\n",
        "    data['body'] = data['body'].str.replace('\\d+', '')\n",
        "    # data['body'] = data['body'].apply(lambda x:vect_dvect.vect_2(x,True,maxlen,max_features))\n",
        "\n",
        "    # data = data.applymap(lambda x:vect(x))\n",
        "    X = data['body'].tolist()\n",
        "    Y = data['heading'].tolist()\n",
        "    \n",
        "    return X, Y\n",
        "\n",
        "X, Y = format_data(data)\n",
        "target = data.pop('heading')\n",
        "data = data.pop('body')\n",
        "\n",
        "\n",
        "train_examples = tf.data.Dataset.from_tensor_slices((data.values, target.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfLBsQnyJ7tn",
        "outputId": "4a873bda-35bf-47ae-86a1-3da59877b1e1"
      },
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "  print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "  print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portuguese:  భారీ ఎత్తున మొండిబకాయిలు పెరిగిపోవడంతో ఐడిబిఐ వ్యవహారాలపై భారత రిజర్వు బ్యాంకు (ఆర్‌బిఐ) ఒక కన్నేసింది. ఐడిబిఐ బ్యాంకును ఆర్‌బిఐ వాచ్‌లి్‌స్టలో చేర్చినట్టుగా తెలిసింది. నికర మొండిపద్దుల మొత్తం 6 శాతం దాటడం, వరసగా రెండేళ్ల పాటు నష్టాలను  ప్రకటించడం, కాపిటల్‌ అడెక్వసీ నిర్దేశిత ప్రమాణాల కంటే తగ్గడం... ఈ సందర్భాల్లో బ్యాంకులను ఆర్‌బిఐ వాచ్‌ లిస్ట్‌లో చేరుస్తుంది. తమ బ్యాంకుకు సంబంధించి ఆర్‌బిఐ ప్రాంప్ట్‌ కరెక్టీవ్‌ యాక్షన్‌ (పిసిఎ) చేపట్టినట్టు ఐడిబిఐ వెల్లడించింది. గతంలో ఓవర్సీస్‌ బ్యాంక్‌, యునైటెడ్‌ బ్యాంక్‌కు సంబంధించి  కూడా ఆర్‌బిఐ పిసిఎ చేపట్టింది. పిసిఎ వల్ల బ్యాంకుపై ఆర్థికంగా భారం పడదనీ, దీనివల్ల అంతర్గత నియంత్రణలు మెరుగుపడటంతో పాటు, కార్యకలాపాలు మెరుగవుతాయని ఐడిబిఐ వివరించింది. ఆర్‌బిఐ ఆదేశాలకు అనుగుణంగా ఇప్పుడు మూలధనాన్ని పొదుపుగా వినియోగించడంపై  ఐడిబిఐ దృష్టి సారించాల్సి ఉంటుంది.ఇందులో భాగంగా పరపతిపై నియంత్రణలు విధించడం, నియామకాలను నిలిపేయడం, కొత్త పెట్టుబడి పథకాలను పక్కన బెట్టడంవంటి చర్యలు తీసుకోవాల్సి ఉంటుం ది. బ్యాంకు సరైన రీతిలో స్పందించని పక్షంలో మరో బలమైన బ్యాంకులో  విలీనానికి  సిద్ధం కావాల్సి ఉంటుంది.\n",
            "English:    ఐడిబిఐపై ఆర్‌బిఐ నజర్‌\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgNKVxieL1x9"
      },
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHLZz9KdGHje",
        "outputId": "8471cbd0-79fe-40ac-e382-e3423b6cc3c2"
      },
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ఐడిబిఐపై ఆర్‌బిఐ నజర్‌\n",
            "బ్యాంకింగ్‌ చీఫ్‌లతో నేడు జైట్లీ భేటీ\n",
            "కీలక వికెట్ తీసిన జడేజా..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9r9OdwkLB1M"
      },
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcVpfz-WIGzL"
      },
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = 8000,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "3HDW2JH_L42N",
        "outputId": "4b66bced-32ef-450b-bd31-46ca5a2d3999"
      },
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py\u001b[0m in \u001b[0;36mbert_vocab_from_dataset\u001b[0;34m(dataset, vocab_size, reserved_tokens, bert_tokenizer_params, learn_params)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0melement_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-be7733b98698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocab = bert_vocab.bert_vocab_from_dataset(\\n    train_examples.batch(1000).prefetch(2),\\n    **bert_vocab_args\\n)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py\u001b[0m in \u001b[0;36mbert_vocab_from_dataset\u001b[0;34m(dataset, vocab_size, reserved_tokens, bert_tokenizer_params, learn_params)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0melement_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The dataset should contain single-tensor elements.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbert_tokenizer_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The dataset should contain single-tensor elements."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWZQbGu3L8cX",
        "outputId": "c704db36-bb8d-4857-9efd-af8634a7b7fc"
      },
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '%', '&', \"'\"]\n",
            "['న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల']\n",
            "['##చచు', '##వస', 'ఎననకలు', 'మందక', 'సవల', 'ఎవరు', 'నటుడు', 'భరగ', 'వధనం', 'ఆసకత']\n",
            "['##౦', '##౧', '##౨', '##౩', '##౪', '##–', '##‘', '##’', '##“', '##”']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnKKDzXEPy3g"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dThIacV5Q9O0"
      },
      "source": [
        "write_vocab_file('pt_vocab.txt', pt_vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZbZxL45RCHd",
        "outputId": "27bfdb51-2129-4fc6-9108-75bd54060ba7"
      },
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 38 s, sys: 238 ms, total: 38.3 s\n",
            "Wall time: 38.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wje3fqPRRPG",
        "outputId": "6b2eeaad-18dc-4c34-baee-a23a04faefd4"
      },
      "source": [
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '%', \"'\", '(', ')']\n",
            "['‘', '’', '##ల', '##న', '##కు', '##ప', '##త', '##క', '##ు', '##ం']\n",
            "['ఆఫరలు', 'ఆశలు', 'ఈస', 'ఊరట', 'కటమరయుడు', 'కడుకు', 'కద', 'కవరటరసల', 'కషటలు', 'గపప']\n",
            "['##ఙ', '##ఛ', '##ఝ', '##ఞ', '##ఢ', '##ృ', '##ౄ', '##–', '##‘', '##’']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5EHVQyXRWyb"
      },
      "source": [
        "write_vocab_file('en_vocab.txt', en_vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv5nZ_aCRgZD"
      },
      "source": [
        "pt_tokenizer = text.BertTokenizer('pt_vocab.txt', **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1CFbXl9Rhvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b9c9c0-9b7d-476f-eb0e-6a35fab4391e"
      },
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  for ex in en_examples:\n",
        "    print(ex.numpy().decode('utf-8'))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ఐడిబిఐపై ఆర్‌బిఐ నజర్‌\n",
            "బ్యాంకింగ్‌ చీఫ్‌లతో నేడు జైట్లీ భేటీ\n",
            "కీలక వికెట్ తీసిన జడేజా..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHSM4m6URqcr",
        "outputId": "68f91a9c-50d0-4de9-c338-16cdd5fa1ec7"
      },
      "source": [
        "# Tokenize the examples -> (batch, word, word-piece)\n",
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "# Merge the word and word-piece axes -> (batch, tokens)\n",
        "token_batch = token_batch.merge_dims(-2,-1)\n",
        "\n",
        "for ex in token_batch.to_list():\n",
        "  print(ex)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[57, 116, 173, 453, 105, 572, 80, 325]\n",
            "[1780, 420, 232, 316, 431, 404]\n",
            "[287, 1154, 1929, 1763, 13, 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560laVRlR8hH",
        "outputId": "fd8d8cda-6b15-485d-8349-9e72c470db34"
      },
      "source": [
        "# Lookup each token id in the vocabulary.\n",
        "txt_tokens = tf.gather(en_vocab, token_batch)\n",
        "# Join with spaces.\n",
        "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'\\xe0\\xb0\\x90 ##\\xe0\\xb0\\xa1 ##\\xe0\\xb0\\xac ##\\xe0\\xb0\\x90 ##\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8 ##\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0',\n",
              "       b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab ##\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f',\n",
              "       b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . .'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Yf3nLzSANp",
        "outputId": "741e3e24-5187-4b20-9d38-e96b6dbdfdee"
      },
      "source": [
        "words = en_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'\\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0',\n",
              "       b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f',\n",
              "       b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . .'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRMlxbqzSFqm"
      },
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IicFn72VSIhC",
        "outputId": "c8c478a2-8610-4bfc-fc4e-d215edb017ad"
      },
      "source": [
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
              "array([b'[START] \\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0 [END]',\n",
              "       b'[START] \\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f [END]',\n",
              "       b'[START] \\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . . [END]'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNKnUkL9SMiD"
      },
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "  # Drop the reserved tokens, except for \"[UNK]\".\n",
        "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "  bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "  # Join them into strings.\n",
        "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDLkJ84dSO1J",
        "outputId": "db14bc0b-2aaa-408d-d24e-8a0053e060cb"
      },
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
        "words = en_tokenizer.detokenize(token_batch)\n",
        "words\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'\\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa', b'\\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90', b'\\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0'], [b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97', b'\\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4', b'\\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81', b'\\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xad\\xe0\\xb0\\x9f'], [b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95', b'\\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f', b'\\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8', b'\\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c', b'.', b'.']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79CxtKY3SZA9",
        "outputId": "3cb7e37c-d4b2-46bf-8f02-f0ac327b95ab"
      },
      "source": [
        "cleanup_text(reserved_tokens, words).numpy()\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'\\xe0\\xb0\\x90\\xe0\\xb0\\xa1\\xe0\\xb0\\xac\\xe0\\xb0\\x90\\xe0\\xb0\\xaa \\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb0\\xac\\xe0\\xb0\\x90 \\xe0\\xb0\\xa8\\xe0\\xb0\\x9c\\xe0\\xb0\\xb0',\n",
              "       b'\\xe0\\xb0\\xac\\xe0\\xb0\\xaf\\xe0\\xb0\\x82\\xe0\\xb0\\x95\\xe0\\xb0\\x82\\xe0\\xb0\\x97 \\xe0\\xb0\\x9a\\xe0\\xb0\\xab\\xe0\\xb0\\xb2\\xe0\\xb0\\xa4 \\xe0\\xb0\\xa8\\xe0\\xb0\\xa1\\xe0\\xb1\\x81 \\xe0\\xb0\\x9c\\xe0\\xb0\\x9f\\xe0\\xb0\\xb2 \\xe0\\xb0\\xad\\xe0\\xb0\\x9f',\n",
              "       b'\\xe0\\xb0\\x95\\xe0\\xb0\\xb2\\xe0\\xb0\\x95 \\xe0\\xb0\\xb5\\xe0\\xb0\\x95\\xe0\\xb0\\x9f \\xe0\\xb0\\xa4\\xe0\\xb0\\xb8\\xe0\\xb0\\xa8 \\xe0\\xb0\\x9c\\xe0\\xb0\\xa1\\xe0\\xb0\\x9c . .'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu-nToxaSgAM"
      },
      "source": [
        "class CustomTokenizer(tf.Module):\n",
        "  def __init__(self, reserved_tokens, vocab_path):\n",
        "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self._reserved_tokens = reserved_tokens\n",
        "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "    self.vocab = tf.Variable(vocab)\n",
        "\n",
        "    ## Create the signatures for export:   \n",
        "\n",
        "    # Include a tokenize signature for a batch of strings. \n",
        "    self.tokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "    # Include `detokenize` and `lookup` signatures for:\n",
        "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "    #   * `RaggedTensors` with shape [batch, tokens]\n",
        "    self.detokenize.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.detokenize.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    self.lookup.get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "    self.lookup.get_concrete_function(\n",
        "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "    # These `get_*` methods take no arguments\n",
        "    self.get_vocab_size.get_concrete_function()\n",
        "    self.get_vocab_path.get_concrete_function()\n",
        "    self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "  @tf.function\n",
        "  def tokenize(self, strings):\n",
        "    enc = self.tokenizer.tokenize(strings)\n",
        "    # Merge the `word` and `word-piece` axes.\n",
        "    enc = enc.merge_dims(-2,-1)\n",
        "    enc = add_start_end(enc)\n",
        "    return enc\n",
        "\n",
        "  @tf.function\n",
        "  def detokenize(self, tokenized):\n",
        "    words = self.tokenizer.detokenize(tokenized)\n",
        "    return cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "  @tf.function\n",
        "  def lookup(self, token_ids):\n",
        "    return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "  @tf.function\n",
        "  def get_vocab_path(self):\n",
        "    return self._vocab_path\n",
        "\n",
        "  @tf.function\n",
        "  def get_reserved_tokens(self):\n",
        "    return tf.constant(self._reserved_tokens)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSKfX2BcShtL",
        "outputId": "c2784795-5185-4c3b-a37e-e594750c908e"
      },
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.bd = CustomTokenizer(reserved_tokens, 'pt_vocab.txt')\n",
        "tokenizers.hd = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NoFy3foSu6-",
        "outputId": "53c3719c-a520-4f0c-a768-e18bcba2b65b"
      },
      "source": [
        "model_name = 'telugu_tokenizer'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: telugu_tokenizer/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF2UEPPPTDGq",
        "outputId": "1c2d0437-3149-4467-b8a8-88fb8aab9f2c"
      },
      "source": [
        "!zip -r /content/tokenizer.zip /content/telugu_tokenizer\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/telugu_tokenizer/ (stored 0%)\n",
            "  adding: content/telugu_tokenizer/assets/ (stored 0%)\n",
            "  adding: content/telugu_tokenizer/assets/pt_vocab.txt (deflated 72%)\n",
            "  adding: content/telugu_tokenizer/assets/en_vocab.txt (deflated 70%)\n",
            "  adding: content/telugu_tokenizer/variables/ (stored 0%)\n",
            "  adding: content/telugu_tokenizer/variables/variables.data-00000-of-00001 (deflated 69%)\n",
            "  adding: content/telugu_tokenizer/variables/variables.index (deflated 33%)\n",
            "  adding: content/telugu_tokenizer/saved_model.pb (deflated 91%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Ycc53sTVX2",
        "outputId": "b2907872-59ef-4275-b3e7-e0fbefe2b0c2"
      },
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
        "reloaded_tokenizers.bd.get_vocab_size().numpy()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7782"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db1GU80uTafF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}