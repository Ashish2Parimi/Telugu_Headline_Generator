{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNy3oCmz9WfToNpzb+3phDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish2Parimi/Telugu_Headline_Generator/blob/main/transformer_stpic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lrkhV34tstz"
      },
      "source": [
        "# !pip install tensorflow_text\n",
        "# !pip install sentencepiece\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PqoYuxqvMmy"
      },
      "source": [
        "# import sentencepiece as spm\n",
        "# spm.SentencePieceTrainer.train('--input=body.txt --model_prefix=body --vocab_size=8000')\n",
        "# spm.SentencePieceTrainer.train('--input=body.txt --model_prefix=heading --vocab_size=2000')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuN0Do6PvMq5"
      },
      "source": [
        "import requests\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "url = 'https://github.com/Ashish2Parimi/Telugu_Headline_Generator/raw/main/body.model'\n",
        "bd_model = requests.get(url).content"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK5pySa1tqGy"
      },
      "source": [
        "import requests\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "url = 'https://github.com/Ashish2Parimi/Telugu_Headline_Generator/raw/main/heading.model'\n",
        "hd_model = requests.get(url).content"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AGpvHdPt1wx"
      },
      "source": [
        "bd_tokenizer = text.SentencepieceTokenizer(bd_model, out_type=tf.string)\n",
        "hd_tokenizer = text.SentencepieceTokenizer(hd_model, out_type=tf.string)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ8dORzk1W5M",
        "outputId": "51b87b49-2b14-450a-e125-bf962cc16372"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "data = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# data = data.drop(['SNo','date','topic'],axis = 1)\n",
        "data.dropna(axis = 0,inplace = True)\n",
        "\n",
        "# data = data.applymap(lambda x:x.encode(encoding='UTF-8'))\n",
        "for i,b in enumerate(data['body'].values):\n",
        "    if len(b)>1000:\n",
        "        data.drop(i, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "def format_data(data):\n",
        "   \n",
        "    \n",
        "    data = data.applymap(lambda x: str(x) if isinstance(x, int) or isinstance(x, float) else x)\n",
        "    data['heading'] = data['heading'].str.replace('\\d+', '')\n",
        "\n",
        "    data['body'] = data['body'].str.replace('\\d+', '')    \n",
        "   \n",
        "    \n",
        "    return data\n",
        "\n",
        "data = format_data(data)\n",
        "data = data[:4000]\n",
        "target = data.pop('heading')\n",
        "data = data.pop('body')\n",
        "import tensorflow as tf\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data.values, target.values))\n",
        "len(dataset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRSrT7Nrfpl5",
        "outputId": "2231ba44-e5f5-44a3-c59e-e0c349176561"
      },
      "source": [
        "val_examples = dataset.take(1000)\n",
        "val_examples\n",
        "train_examples = dataset.skip(1000)\n",
        "train_examples "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SkipDataset shapes: ((), ()), types: (tf.string, tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm5AnEFdyRxY",
        "outputId": "5424cf46-7c38-4d92-cd5a-d90de3ed51c9"
      },
      "source": [
        "for bd_examples, hd_examples in train_examples.batch(3).take(1):\n",
        "  for bd in bd_examples.numpy():\n",
        "    print(bd.decode('utf-8'))\n",
        "    print(len(bd.decode('utf-8')))\n",
        "\n",
        "\n",
        "  print()\n",
        "\n",
        "  for hd in hd_examples.numpy():\n",
        "    print(hd.decode('utf-8'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "న్యూఢిల్లీ, మే : ఏడవ వేతన సంఘం సిఫారసు చేసిన మేరకు భత్యాలను పెంచాలన్న డిమాండ్‌తో ఈ నెల న తలపెట్టిన ఒక రోజు సమ్మెను కేంద్ర ప్రభుత్వ ఉద్యోగ సంఘాలు వాయిదా వేశాయి. వేతన సంఘం సిఫారసు చేసిన మేరకు భత్యాలు పెంచేందుకు ప్రభుత్వం హామీ ఇచ్చిందని, ఈ నేపథ్యంలో సమ్మెను వాయిదా వేశామని కేంద్ర ప్రభుత్వ ఉద్యోగుల సమాఖ్య ఓ ప్రకటనలో తెలిపింది.\n",
            "323\n",
            "న్యూఢిల్లీ, జనవరి : అమెరికా కొత్త అధ్యక్షుడుగా బాధ్యతలు చేపట్టిన డోనాల్డ్‌ ట్రంప్‌కు ప్రధాని మోదీ అభినందనలు తెలిపారు. శుక్రవారం రాత్రి ఆయన ట్వీట్‌ చేస్తూ, అమెరికాతో ద్వైపాక్షిక సంబంధాలు మరింత పటిష్ఠం కావాలని, సహకారం ఇంకా విస్తృతం కావాలని ఆకాంక్షించారు.\n",
            "252\n",
            "న్యూఢిల్లీ, జూలై  (ఆంధ్రజ్యోతి): కేంద్ర పట్టణాభివృద్ధి శాఖమంత్రిగా రాజీనామ చేసిన ఎన్డీయే ఉప రాష్ట్రపతి అభ్యర్థి ఎం.వెంకయ్యనాయుడికు ఆ శాఖ ఉద్యోగులు సోమవారం ఘనంగా వీడ్కోలు పలికారు. వెంకయ్య మాట్లాడుతూ.. మూడేళ్లలో శాఖలో అందరితో అనుబంధం పెరిగిందని, అందరికీ అభినందనలు తెలిపారు.\n",
            "271\n",
            "\n",
            "న కేంద్ర ఉద్యోగుల సమ్మె లేదు\n",
            "ట్రంప్‌కు మోదీ అభినందనలు \n",
            "వెంకయ్యకు ఘనంగా వీడ్కోలు\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEmDuhs0y4sK",
        "outputId": "c48a16ac-756e-458c-be86-be3a74b306e8"
      },
      "source": [
        "encoded = hd_tokenizer.tokenize(hd_examples)\n",
        "encoded = hd_tokenizer.string_to_id(encoded)\n",
        "\n",
        "for row in encoded.to_list():\n",
        "  print(row)\n",
        "  \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[52, 174, 774, 182, 45, 232, 79, 648]\n",
            "[264, 590, 83, 284, 784, 16, 124, 16, 22]\n",
            "[228, 6, 30, 154, 18, 980, 48, 530, 92, 237, 22]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SvNtpapzNaT",
        "outputId": "ab9d3a4c-5219-401b-fd97-726225bf486f"
      },
      "source": [
        "round_trip = hd_tokenizer.detokenize(encoded)\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "న కేంద్ర ఉద్యోగుల సమ్మె లేదు\n",
            "ట్రంప్ కు మోదీ అభినందనలు\n",
            "వెంకయ్యకు ఘనంగా వీడ్కోలు\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7u5Oua-5PNw"
      },
      "source": [
        "def tokenize_pairs(bd, hd):\n",
        "    bd = bd_tokenizer.tokenize(bd)\n",
        "    bd = bd_tokenizer.string_to_id(bd)\n",
        "\n",
        "    # Convert from ragged to dense, padding with zeros.\n",
        "    bd = tf.dtypes.cast(bd.to_tensor(),dtype=tf.int64)\n",
        "\n",
        "    hd = hd_tokenizer.tokenize(hd)\n",
        "    hd = hd_tokenizer.string_to_id(hd)\n",
        "\n",
        "    # Convert from ragged to dense, padding with zeros.\n",
        "    hd = tf.dtypes.cast(hd.to_tensor(),dtype=tf.int64)\n",
        "    return bd, hd"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5to2UKMU5vmS"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRK87iSP52lQ"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9i2iAtT54GR"
      },
      "source": [
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8R7m5BvIIUv",
        "outputId": "4cc450f1-4ad8-43bb-c802-9ee9aec062ad"
      },
      "source": [
        " for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "   print(inp)\n",
        "   print(tar)\n",
        "\n",
        "   break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  67    0    7 ...    0    0    0]\n",
            " [  62   81   52 ...    0    0    0]\n",
            " [  67    7    0 ...    0    0    0]\n",
            " ...\n",
            " [  80 1015  318 ...    0    0    0]\n",
            " [  67    0  278 ...    0    0    0]\n",
            " [3816 3518    7 ...    0    0    0]], shape=(64, 337), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[  97  480 1332 ...    0    0    0]\n",
            " [1055   92  680 ...    0    0    0]\n",
            " [ 668    9  682 ...    0    0    0]\n",
            " ...\n",
            " [  56  810   50 ...    0    0    0]\n",
            " [ 243    8   37 ...    0    0    0]\n",
            " [  73  107   29 ...    0    0    0]], shape=(64, 25), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5yNLTjK5_RU"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JYuFQeC6KEE"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6JNulI_6LwJ"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAuKabZG6Oqp"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl8Inwo16T0G"
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print('Attention weights are:')\n",
        "  print(temp_attn)\n",
        "  print('Output is:')\n",
        "  print(temp_out)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IemyYhh6U2p"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlxeYaMS6Yfz"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwhZU_ZX6cNC"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40BybZGH6e56"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2JAKvw26kaz"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_QxZ4F86m_j"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhnK1ldh6oHU"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inp, tar, training, enc_padding_mask,\n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1x5WqXz6tYz"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i30T6ZjA6zTI"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uULQomCC60px"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33wTDJszGgBL"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giMDqhjdGidX"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBpkBGsTHY_I"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lYp4In7HbYk"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=bd_tokenizer.vocab_size(),\n",
        "    target_vocab_size=hd_tokenizer.vocab_size(),\n",
        "    pe_input=1000,\n",
        "    pe_target=100,\n",
        "    rate=dropout_rate)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VlHUzTVH9s8"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by\n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EdfpFpVICa1"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsqBRetsIFQG"
      },
      "source": [
        "EPOCHS = 50"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Y7APmcIGKi"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,\n",
        "                                 True,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kou1RjYIOWg",
        "outputId": "6c971980-3662-43d1-b681-c1eed2184660"
      },
      "source": [
        "import time\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 7.6697 Accuracy 0.0000\n",
            "Epoch 1 Loss 7.6206 Accuracy 0.0007\n",
            "Time taken for 1 epoch: 24.36 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.5145 Accuracy 0.0029\n",
            "Epoch 2 Loss 7.4346 Accuracy 0.0077\n",
            "Time taken for 1 epoch: 9.58 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 7.3488 Accuracy 0.0085\n",
            "Epoch 3 Loss 7.2474 Accuracy 0.0147\n",
            "Time taken for 1 epoch: 9.47 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 7.1344 Accuracy 0.0096\n",
            "Epoch 4 Loss 7.0283 Accuracy 0.0190\n",
            "Time taken for 1 epoch: 9.59 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 6.8827 Accuracy 0.0172\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 6.7777 Accuracy 0.0202\n",
            "Time taken for 1 epoch: 9.95 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 6.7008 Accuracy 0.0207\n",
            "Epoch 6 Loss 6.5424 Accuracy 0.0200\n",
            "Time taken for 1 epoch: 9.65 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 6.4382 Accuracy 0.0175\n",
            "Epoch 7 Loss 6.3635 Accuracy 0.0207\n",
            "Time taken for 1 epoch: 9.69 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 6.3090 Accuracy 0.0221\n",
            "Epoch 8 Loss 6.2629 Accuracy 0.0225\n",
            "Time taken for 1 epoch: 9.63 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 6.2171 Accuracy 0.0290\n",
            "Epoch 9 Loss 6.2144 Accuracy 0.0258\n",
            "Time taken for 1 epoch: 9.61 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 6.1800 Accuracy 0.0277\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 6.1776 Accuracy 0.0289\n",
            "Time taken for 1 epoch: 9.81 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 6.1432 Accuracy 0.0507\n",
            "Epoch 11 Loss 6.1325 Accuracy 0.0386\n",
            "Time taken for 1 epoch: 9.58 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 6.0751 Accuracy 0.0488\n",
            "Epoch 12 Loss 6.0623 Accuracy 0.0451\n",
            "Time taken for 1 epoch: 9.65 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 5.9913 Accuracy 0.0584\n",
            "Epoch 13 Loss 5.9733 Accuracy 0.0571\n",
            "Time taken for 1 epoch: 9.78 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 5.9228 Accuracy 0.0753\n",
            "Epoch 14 Loss 5.8628 Accuracy 0.0682\n",
            "Time taken for 1 epoch: 9.68 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 5.8914 Accuracy 0.0526\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 5.7326 Accuracy 0.0790\n",
            "Time taken for 1 epoch: 9.93 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 5.7128 Accuracy 0.0840\n",
            "Epoch 16 Loss 5.5941 Accuracy 0.0916\n",
            "Time taken for 1 epoch: 9.61 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 5.5271 Accuracy 0.0847\n",
            "Epoch 17 Loss 5.4405 Accuracy 0.1096\n",
            "Time taken for 1 epoch: 9.74 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 5.3040 Accuracy 0.1153\n",
            "Epoch 18 Loss 5.2971 Accuracy 0.1223\n",
            "Time taken for 1 epoch: 9.79 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 5.2001 Accuracy 0.1269\n",
            "Epoch 19 Loss 5.1535 Accuracy 0.1349\n",
            "Time taken for 1 epoch: 9.71 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 5.0400 Accuracy 0.1606\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 5.0053 Accuracy 0.1505\n",
            "Time taken for 1 epoch: 9.98 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 4.8602 Accuracy 0.1764\n",
            "Epoch 21 Loss 4.8613 Accuracy 0.1668\n",
            "Time taken for 1 epoch: 9.78 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 4.6994 Accuracy 0.1987\n",
            "Epoch 22 Loss 4.7268 Accuracy 0.1805\n",
            "Time taken for 1 epoch: 9.69 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 4.4524 Accuracy 0.2079\n",
            "Epoch 23 Loss 4.5858 Accuracy 0.1932\n",
            "Time taken for 1 epoch: 9.72 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 4.6082 Accuracy 0.1872\n",
            "Epoch 24 Loss 4.4501 Accuracy 0.2079\n",
            "Time taken for 1 epoch: 9.72 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 4.3376 Accuracy 0.2434\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 4.3097 Accuracy 0.2259\n",
            "Time taken for 1 epoch: 10.03 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 4.1423 Accuracy 0.2465\n",
            "Epoch 26 Loss 4.1720 Accuracy 0.2403\n",
            "Time taken for 1 epoch: 9.74 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 4.0468 Accuracy 0.2674\n",
            "Epoch 27 Loss 4.0369 Accuracy 0.2576\n",
            "Time taken for 1 epoch: 9.84 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 3.8607 Accuracy 0.2640\n",
            "Epoch 28 Loss 3.8885 Accuracy 0.2734\n",
            "Time taken for 1 epoch: 9.82 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 3.6715 Accuracy 0.3075\n",
            "Epoch 29 Loss 3.7638 Accuracy 0.2909\n",
            "Time taken for 1 epoch: 9.80 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 3.5155 Accuracy 0.3250\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 3.6248 Accuracy 0.3071\n",
            "Time taken for 1 epoch: 9.92 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 3.6190 Accuracy 0.2975\n",
            "Epoch 31 Loss 3.4796 Accuracy 0.3258\n",
            "Time taken for 1 epoch: 9.79 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 3.4427 Accuracy 0.3170\n",
            "Epoch 32 Loss 3.3324 Accuracy 0.3450\n",
            "Time taken for 1 epoch: 9.88 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 3.1487 Accuracy 0.3694\n",
            "Epoch 33 Loss 3.2046 Accuracy 0.3607\n",
            "Time taken for 1 epoch: 9.89 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 3.0435 Accuracy 0.3819\n",
            "Epoch 34 Loss 3.0660 Accuracy 0.3828\n",
            "Time taken for 1 epoch: 9.85 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 2.8658 Accuracy 0.4457\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 2.9291 Accuracy 0.4038\n",
            "Time taken for 1 epoch: 10.05 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 2.7144 Accuracy 0.4385\n",
            "Epoch 36 Loss 2.8030 Accuracy 0.4221\n",
            "Time taken for 1 epoch: 9.76 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 2.6313 Accuracy 0.4552\n",
            "Epoch 37 Loss 2.6446 Accuracy 0.4459\n",
            "Time taken for 1 epoch: 9.89 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 2.4818 Accuracy 0.4590\n",
            "Epoch 38 Loss 2.4989 Accuracy 0.4686\n",
            "Time taken for 1 epoch: 9.75 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 2.3346 Accuracy 0.5139\n",
            "Epoch 39 Loss 2.3806 Accuracy 0.4887\n",
            "Time taken for 1 epoch: 9.94 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 2.0661 Accuracy 0.5533\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 2.2523 Accuracy 0.5114\n",
            "Time taken for 1 epoch: 10.12 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.8447 Accuracy 0.5754\n",
            "Epoch 41 Loss 2.1267 Accuracy 0.5319\n",
            "Time taken for 1 epoch: 9.93 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 2.1145 Accuracy 0.5112\n",
            "Epoch 42 Loss 1.9850 Accuracy 0.5602\n",
            "Time taken for 1 epoch: 9.85 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 1.7240 Accuracy 0.6110\n",
            "Epoch 43 Loss 1.8687 Accuracy 0.5783\n",
            "Time taken for 1 epoch: 9.78 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 1.7632 Accuracy 0.5928\n",
            "Epoch 44 Loss 1.7534 Accuracy 0.5995\n",
            "Time taken for 1 epoch: 9.79 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 1.6018 Accuracy 0.6373\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 1.6322 Accuracy 0.6198\n",
            "Time taken for 1 epoch: 10.02 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 1.4851 Accuracy 0.6617\n",
            "Epoch 46 Loss 1.5231 Accuracy 0.6431\n",
            "Time taken for 1 epoch: 9.87 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 1.4653 Accuracy 0.6346\n",
            "Epoch 47 Loss 1.3954 Accuracy 0.6705\n",
            "Time taken for 1 epoch: 9.80 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 1.2490 Accuracy 0.6959\n",
            "Epoch 48 Loss 1.3127 Accuracy 0.6850\n",
            "Time taken for 1 epoch: 9.84 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 1.1595 Accuracy 0.7116\n",
            "Epoch 49 Loss 1.2093 Accuracy 0.7129\n",
            "Time taken for 1 epoch: 9.83 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 1.0223 Accuracy 0.7407\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 1.1212 Accuracy 0.7266\n",
            "Time taken for 1 epoch: 10.16 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ2yUqMbpSzq",
        "outputId": "eb5892f8-87ac-4563-f42f-a71ce79ea091"
      },
      "source": [
        "for bd_examples, hd_examples in train_examples.batch(1).take(1):\n",
        "  for bd in bd_examples.numpy():\n",
        "    print(bd.decode('utf-8'))\n",
        "    sentence = bd.decode('utf-8')\n",
        "  print()\n",
        "\n",
        "  for hd in hd_examples.numpy():\n",
        "    print(hd.decode('utf-8'))\n",
        "    ground_truth = hd.decode('utf-8')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "న్యూఢిల్లీ, మే : ఏడవ వేతన సంఘం సిఫారసు చేసిన మేరకు భత్యాలను పెంచాలన్న డిమాండ్‌తో ఈ నెల న తలపెట్టిన ఒక రోజు సమ్మెను కేంద్ర ప్రభుత్వ ఉద్యోగ సంఘాలు వాయిదా వేశాయి. వేతన సంఘం సిఫారసు చేసిన మేరకు భత్యాలు పెంచేందుకు ప్రభుత్వం హామీ ఇచ్చిందని, ఈ నేపథ్యంలో సమ్మెను వాయిదా వేశామని కేంద్ర ప్రభుత్వ ఉద్యోగుల సమాఖ్య ఓ ప్రకటనలో తెలిపింది.\n",
            "\n",
            "న కేంద్ర ఉద్యోగుల సమ్మె లేదు\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKkxxrUFr3A2"
      },
      "source": [
        "sentence = tf.convert_to_tensor([sentence])\n",
        "sentence = bd_tokenizer.tokenize(sentence)\n",
        "sentence = bd_tokenizer.string_to_id(sentence)\n",
        "sentence = tf.dtypes.cast(sentence.to_tensor(),dtype=tf.int64)\n",
        "encoder_input = sentence"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVJ-W4j_r915"
      },
      "source": [
        "start= hd_tokenizer.tokenize([''])[0]\n",
        "output = tf.convert_to_tensor([start])\n",
        "output = tf.expand_dims(output, 0)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "hBtarAoTsE2e",
        "outputId": "45ef7a37-6ec6-48ec-bc8a-221ade7f0e93"
      },
      "source": [
        "\n",
        "for i in range(40):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "          encoder_input, output)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                output,\n",
        "\n",
        "                                                False,\n",
        "                                                enc_padding_mask,\n",
        "                                                combined_mask,\n",
        "                                                dec_padding_mask)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-48ccf2c1c7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n\u001b[0;32m----> 4\u001b[0;31m           encoder_input, output)\n\u001b[0m\u001b[1;32m      5\u001b[0m     predictions, attention_weights = transformer(encoder_input,\n\u001b[1;32m      6\u001b[0m                                                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-31c624093747>\u001b[0m in \u001b[0;36mcreate_masks\u001b[0;34m(inp, tar)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# the decoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlook_ahead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_look_ahead_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mdec_target_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mcombined_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_target_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-74fec66be396>\u001b[0m in \u001b[0;36mcreate_padding_mask\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# add extra dimensions to add the padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# to the attention logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mof\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mincompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m   \"\"\"\n\u001b[0;32m-> 1679\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[1;32m   3159\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3160\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Equal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"incompatible_shape_error\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         incompatible_shape_error)\n\u001b[0m\u001b[1;32m   3162\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert 0 to EagerTensor of dtype string"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s_AL9W9qq12"
      },
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  # print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcxP2RGlqw0B",
        "outputId": "dfdcd105-e37f-4a4f-e6bf-bfbbc5c82c10"
      },
      "source": [
        "print_translation(sentence, ground_truth)\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:         : న్యూఢిల్లీ, మే : ఏడవ వేతన సంఘం సిఫారసు చేసిన మేరకు భత్యాలను పెంచాలన్న డిమాండ్‌తో ఈ నెల న తలపెట్టిన ఒక రోజు సమ్మెను కేంద్ర ప్రభుత్వ ఉద్యోగ సంఘాలు వాయిదా వేశాయి. వేతన సంఘం సిఫారసు చేసిన మేరకు భత్యాలు పెంచేందుకు ప్రభుత్వం హామీ ఇచ్చిందని, ఈ నేపథ్యంలో సమ్మెను వాయిదా వేశామని కేంద్ర ప్రభుత్వ ఉద్యోగుల సమాఖ్య ఓ ప్రకటనలో తెలిపింది.\n",
            "Ground truth   : న కేంద్ర ఉద్యోగుల సమ్మె లేదు\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqIoL8wYncpc"
      },
      "source": [
        "def evaluate(sentence, max_length=40):\n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  sentence = tf.convert_to_tensor([sentence])\n",
        "  sentence = bd_tokenizer.tokenize(sentence)\n",
        "  sentence = bd_tokenizer.string_to_id(sentence)\n",
        "  sentence = tf.dtypes.cast(sentence.to_tensor(),dtype=tf.int64)\n",
        "  encoder_input = sentence\n",
        "\n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  start= hd_tokenizer.tokenize([''])[0]\n",
        "  output = tf.convert_to_tensor([start])\n",
        "  output = tf.expand_dims(output, 0)\n",
        "\n",
        "  for i in range(max_length):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input,\n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == end:\n",
        "      break\n",
        "\n",
        "  # output.shape (1, tokens)\n",
        "  text = hd_tokenizer.detokenize(output)  # shape: ()\n",
        "\n",
        "\n",
        "  return text, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "FTkU2cv8rUIP",
        "outputId": "187cf9f8-b81c-4a9b-c289-63376b9a41d8"
      },
      "source": [
        "translated_text, attention_weights = evaluate(sentence)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-1ad1d15a97e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslated_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-8ca9fc2dc5d9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence, max_length)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n\u001b[0;32m---> 17\u001b[0;31m         encoder_input, output)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# predictions.shape == (batch_size, seq_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-31c624093747>\u001b[0m in \u001b[0;36mcreate_masks\u001b[0;34m(inp, tar)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# the decoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlook_ahead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_look_ahead_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mdec_target_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mcombined_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_target_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-74fec66be396>\u001b[0m in \u001b[0;36mcreate_padding_mask\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# add extra dimensions to add the padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# to the attention logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mof\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mincompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m   \"\"\"\n\u001b[0;32m-> 1679\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[1;32m   3159\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3160\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Equal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"incompatible_shape_error\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         incompatible_shape_error)\n\u001b[0m\u001b[1;32m   3162\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert 0 to EagerTensor of dtype string"
          ]
        }
      ]
    }
  ]
}