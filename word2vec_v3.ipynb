{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNx+mrnkLByC/rnANCySQqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish2Parimi/Telugu_Headline_Generator/blob/main/word2vec_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtuqB9HpAqCY"
      },
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnud7xeAu1N"
      },
      "source": [
        "data = pd.read_csv('/content/data.csv')\n",
        "data = data.applymap(lambda x: str(x) if isinstance(x, int) or isinstance(x, float) else x)\n",
        "\n",
        "d = list(data['heading']+' '+data['body'])\n",
        "d = ' '.join(d)\n",
        "text_file = open(\"data.txt\", \"w\")\n",
        "n = text_file.write(d)\n",
        "text_file.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeXPtfnMGd0R"
      },
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkOxZLe3FI_9"
      },
      "source": [
        "path_to_file = '/content/data.txt'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8XsueJhBt1C",
        "outputId": "9b9b312d-9850-4a67-8997-0bd3c7637843"
      },
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "text_ds"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<FilterDataset shapes: (), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26XrdLi7HDAP"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=SEED,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhRM1knwDTfK"
      },
      "source": [
        "# Now, create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re\n",
        "import string\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QfEtd58DdmO"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dfSsNnfG0BY"
      },
      "source": [
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3XhTdrQCyqx"
      },
      "source": [
        "vocab = vectorize_layer.get_vocabulary()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97crqgL3C_fN"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckrtVdmeDAwx"
      },
      "source": [
        "write_vocab_file('vocab.txt', vocab)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3WjTN8_HKAd",
        "outputId": "d0f48779-8435-4fbe-f3c4-196f031107a8"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "CXV22R49HXlV",
        "outputId": "3c9cd832-9684-44a9-919d-5a46b4156f62"
      },
      "source": [
        "# for seq in sequences[:5]:\n",
        "#   print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-89b91f85264d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{seq} => {[inverse_vocab[i] for i in seq]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-89b91f85264d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{seq} => {[inverse_vocab[i] for i in seq]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'inverse_vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hriGytHBHba3",
        "outputId": "47bbab72-e4bb-4481-f4ab-4a17cabb5b02"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=2,\n",
        "    num_ns=4,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3886/3886 [00:07<00:00, 513.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16197 16197 16197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmQkajjfHk2J",
        "outputId": "1ca6b5b7-874e-41f0-f7f0-0f7411a7fd98"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4p0bJQ5Hriq",
        "outputId": "9f2c2514-565e-42fc-9d63-35f4abd697d1"
      },
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-83XEh9HwZ1"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3, 2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = self.dots([context_emb, word_emb])\n",
        "    return self.flatten(dots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dAVmThjH1zS"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiuUePbEH7Jk"
      },
      "source": [
        "embedding_dim = 128\n",
        "num_ns = 4\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWCy8jGlH8py"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmIQjJY6IUcL",
        "outputId": "066263c6-61b3-426b-df40-96c26c855c68"
      },
      "source": [
        "word2vec.fit(dataset, epochs=50, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "15/15 [==============================] - 4s 45ms/step - loss: 1.6087 - accuracy: 0.2424\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.5909 - accuracy: 0.7616\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.5624 - accuracy: 0.8816\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.5094 - accuracy: 0.8798\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.4234 - accuracy: 0.8571\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.3067 - accuracy: 0.8350\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1755 - accuracy: 0.8156\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0525 - accuracy: 0.8025\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.9531 - accuracy: 0.7942\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.8791 - accuracy: 0.7912\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.8248 - accuracy: 0.7888\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7833 - accuracy: 0.7905\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7498 - accuracy: 0.7926\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.7210 - accuracy: 0.7960\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6954 - accuracy: 0.8008\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6718 - accuracy: 0.8045\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6495 - accuracy: 0.8083\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6281 - accuracy: 0.8135\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6073 - accuracy: 0.8206\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5869 - accuracy: 0.8268\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5667 - accuracy: 0.8334\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5467 - accuracy: 0.8408\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5269 - accuracy: 0.8491\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5072 - accuracy: 0.8557\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4877 - accuracy: 0.8650\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4683 - accuracy: 0.8743\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4492 - accuracy: 0.8830\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4304 - accuracy: 0.8902\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4120 - accuracy: 0.8984\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3940 - accuracy: 0.9049\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3765 - accuracy: 0.9144\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3596 - accuracy: 0.9220\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3432 - accuracy: 0.9291\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3275 - accuracy: 0.9361\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3125 - accuracy: 0.9417\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2982 - accuracy: 0.9473\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2846 - accuracy: 0.9515\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2717 - accuracy: 0.9566\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2596 - accuracy: 0.9615\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2481 - accuracy: 0.9656\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2373 - accuracy: 0.9701\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2271 - accuracy: 0.9726\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2175 - accuracy: 0.9754\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2086 - accuracy: 0.9779\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2002 - accuracy: 0.9795\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1923 - accuracy: 0.9817\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1850 - accuracy: 0.9831\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1781 - accuracy: 0.9850\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1717 - accuracy: 0.9868\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1657 - accuracy: 0.9878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f131de50890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbC3r3O4Il0o"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgGZCvu1JK1O"
      },
      "source": [
        "# %reload_ext tensorboard\n",
        "\n",
        "# %tensorboard --logdir logs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVoSuPAgI5XS"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "word_vec = {}\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  \n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "  word_vec[word] = vec\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itF-tqsILmtM"
      },
      "source": [
        "# word2vec.save(\"word2vec\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkMcN5PL6f1"
      },
      "source": [
        "# try:\n",
        "#   from google.colab import files\n",
        "#   files.download('vectors.tsv')\n",
        "#   files.download('metadata.tsv')\n",
        "#   files.download('/content/word2vec')\n",
        "# except Exception:\n",
        "#   pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IW25_FS0AZC"
      },
      "source": [
        "word_vec "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ce-rFJL7R-A"
      },
      "source": [
        "import csv\n",
        "with open('word_vec.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in word_vec.items():\n",
        "       writer.writerow([key, value])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}